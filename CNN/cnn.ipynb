{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Practica 2. Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "Hernandez Martinez Mally Samira | Código: 220286113 | Ingenieria de computacion (INCO) \n",
    "\n",
    "Seminario de Solucion de Problemas de Inteligencia Artificial II | Seccion D05 I7041\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Caso de estudio</u>:\n",
    "\n",
    "Implementación de un clasificador automático de imágenes deportivas que pueda distinguir entre diferentes disciplinas, como fútbol, baloncesto, atletismo, y natación. Este sistema podría ser utilizado por plataformas de transmisión deportiva para etiquetar contenidos automáticamente en tiempo real, con el fin de mejorar la accesibilidad y búsqueda de contenido por parte de los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tPreprocesamiento de Datos:\n",
    "\n",
    "    - Cargar un conjunto de datos de imágenes deportivas, con múltiples clases que representan diferentes deportes (ej. fútbol, baloncesto, atletismo, natación, etc.).\n",
    "\n",
    "    - Asegurarse de que las imágenes estén etiquetadas correctamente para las distintas disciplinas deportivas.\n",
    "\n",
    "    - Redimensionar las imágenes a un tamaño adecuado para el modelo CNN (ej. 128x128 o 224x224 píxeles).\n",
    "\n",
    "    - Normalizar los datos de entrada para mejorar la convergencia del modelo, dividiendo los valores de los píxeles por 255 (para que queden entre 0 y 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerías\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sklearn\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as ft\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l2\n",
    "from keras.applications import VGG16 \n",
    "from keras.preprocessing import image\n",
    "from keras.utils import load_img, img_to_array \n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import UnidentifiedImageError \n",
    "from contextlib import contextmanager\n",
    "from keras.optimizers import Adam \n",
    "from tensorflow import keras\n",
    "from PIL import ImageFile\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contexto para suprimir la salida\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Definir rutas de datos\n",
    "train_dir = 'C:/Users/mally/Documents/Proyectos/sem_IA2/CNN/dataset'\n",
    "test_dir = 'C:/Users/mally/Documents/Proyectos/sem_IA2/CNN/single_test'\n",
    "\n",
    "# Parámetros\n",
    "img_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Generador de datos con aumento para el entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,  #\n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2,  \n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# Suprimir la salida de los generadores\n",
    "with suppress_stdout():\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "# Información sobre las clases\n",
    "print(f\"Clases detectadas:\\n{train_generator.class_indices}\")\n",
    "print(f\"Imágenes en el conjunto de entrenamiento: {train_generator.samples}\")\n",
    "print(f\"Imágenes en el conjunto de validación: {validation_generator.samples}\")\n",
    "print(f\"Imágenes en el conjunto de prueba: {test_generator.samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tDefinición del Modelo:\n",
    "\n",
    "a) Prueba 1: Modelo CNN desde cero\n",
    "- Crear una red CNN personalizada utilizando varias capas convolucionales y de pooling.\n",
    "- Las capas deben incluir filtros de distintas profundidades (ej. 32, 64, 128), con funciones de activación ReLU para las - capas convolucionales.\n",
    "- Usar Batch Normalization para estabilizar el entrenamiento y mejorar la generalización del modelo.\n",
    "- Aplicar capas de MaxPooling para reducir la dimensionalidad.\n",
    "- Aplicar capas Dropout, para evitar el overffitting.\n",
    "- Añadir capas densas al final, y la última capa debe tener tantas neuronas como clases deportivas, utilizando la función de activación softmax.\n",
    "- Compilar el modelo utilizando un optimizador como Adam y la función de pérdida categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo CNN personalizado\n",
    "model_cnn = Sequential()\n",
    "\n",
    "# Capas convolucionales y de pooling\n",
    "model_cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model_cnn.add(BatchNormalization())\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(BatchNormalization())\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_cnn.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model_cnn.add(BatchNormalization())\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_cnn.add(Conv2D(128, (3, 3), activation='relu'))  \n",
    "model_cnn.add(BatchNormalization())\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Capas densas con regularización L2 y Dropout\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(256, activation='relu', kernel_regularizer=l2(0.0001)))  \n",
    "model_cnn.add(Dropout(0.3))\n",
    "model_cnn.add(Dense(train_generator.num_classes, activation='softmax'))\n",
    "\n",
    "# Cargar el modelo guardado\n",
    "model_cnn = load_model('C:/Users/mally/Documents/Proyectos/sem_IA2/CNN/mejor_modelo.h5')\n",
    "\n",
    "# Compilar el modelo con tasa de aprendizaje reducida\n",
    "model_cnn.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Checkpoint para guardar el mejor modelo\n",
    "checkpoint = ModelCheckpoint('mejor_modelo.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Early stopping para evitar sobreentrenamiento\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=5,  \n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo con una tasa de aprendizaje más baja\n",
    "model_cnn.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks: early stopping, checkpoint y reducción de LR\n",
    "checkpoint = ModelCheckpoint('mejor_modelo.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history_cnn = model_cnn.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=25,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Prueba 2: Modelo con Transfer Learning\n",
    "- Utilizar un modelo preentrenado (ej. VGG16, ResNet50, InceptionV3) entrenado en ImageNet, cargando sus pesos preentrenados.   \n",
    "- Congelar las capas convolucionales del modelo preentrenado para conservar las características ya aprendidas.\n",
    "- Agregar nuevas capas densas al final del modelo, adaptando la última capa para que tenga tantas neuronas como clases deportivas con la función de activación softmax.\n",
    "- Compilar este modelo también con un optimizador como Adam y la función de pérdida categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "img_size = (128,128)  \n",
    "batch_size = 32\n",
    "\n",
    "with suppress_stdout():\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "# Cargar el modelo preentrenado VGG16 (sin la capa superior) y congelar sus capas\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False  # Congelar todas las capas convolucionales\n",
    "\n",
    "# Definir un nuevo modelo secuencial\n",
    "model_transfer = Sequential()\n",
    "\n",
    "# Añadir el modelo base (VGG16 preentrenado)\n",
    "model_transfer.add(base_model)\n",
    "\n",
    "# Aplanar la salida para conectarla a las capas densas\n",
    "model_transfer.add(Flatten())\n",
    "\n",
    "# Añadir una capa completamente conectada (densa)\n",
    "model_transfer.add(Dense(128, activation='relu'))\n",
    "model_transfer.add(Dropout(0.5))  # Dropout para evitar overfitting\n",
    "\n",
    "# Añadir la capa de salida con softmax para la clasificación\n",
    "model_transfer.add(Dense(train_generator.num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model_transfer.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Calcular pesos de clases para manejar clases desbalanceadas\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Revisar si `train_generator.classes` contiene las clases correctas\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),  \n",
    "    y=train_generator.classes  \n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))  \n",
    "\n",
    "history_transfer= model_transfer.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=25,\n",
    "    class_weight=class_weights  \n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model_transfer.save('mejor_transfer_learning.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tAjuste de Hiperparámetros y Fine-tuning:\n",
    "\n",
    "    - Para el modelo CNN desde cero, ajustar el número de capas, neuronas por capa, tasa de aprendizaje y épocas de entrenamiento, esto hasta lograr los resultados esperados en términos de precisión (Accuracy).\n",
    "\n",
    "    - Para el modelo de Transfer Learning, después de las primeras épocas de entrenamiento, descongelar algunas capas superiores del modelo preentrenado y realizar fine-tuning, ajustando el modelo a los datos deportivos, hasta lograr los resultados esperados en términos de precisión (Accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tEvaluación y Métricas:\n",
    "\n",
    "    - Evaluar ambos modelos utilizando métricas como precisión, recall, F1-score y la pérdida en los datos de entrenamiento y validación.\n",
    "\n",
    "    - Generar y presentar la matriz de confusión para analizar los errores de clasificación por cada deporte.\n",
    "\n",
    "    - Graficar la evolución de la precisión y la pérdida durante el entrenamiento y la validación de ambos modelos.\n",
    "\n",
    "    - Predecir de forma correcta al menos 6 de las 10 imágenes establecidas en la carpeta de single_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo CNN y obtener las predicciones\n",
    "Y_pred_cnn = model_cnn.predict(validation_generator, steps=len(validation_generator), verbose=1)\n",
    "y_pred_cnn_classes = np.argmax(Y_pred_cnn, axis=1)  \n",
    "\n",
    "# Verifica las dimensiones de los datos\n",
    "print(\"Shape del validation_generator:\", validation_generator.samples)\n",
    "print(\"Shape de las predicciones del modelo:\", Y_pred_cnn.shape)\n",
    "\n",
    "# Evaluar el modelo de Transfer Learning (si lo tienes)\n",
    "Y_pred_transfer = model_transfer.predict(validation_generator, steps=len(validation_generator), verbose=1)\n",
    "y_pred_transfer_classes = np.argmax(Y_pred_transfer, axis=1)  \n",
    "\n",
    "# Obtener las etiquetas verdaderas\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "# Asegúrate de que las dimensiones de las predicciones coincidan con las etiquetas verdaderas\n",
    "print(\"Dimensiones de las predicciones CNN:\", y_pred_cnn_classes.shape)\n",
    "print(\"Dimensiones de las etiquetas verdaderas:\", y_true.shape)\n",
    "\n",
    "# Asegúrate de que las dimensiones coincidan antes de continuar con la evaluación\n",
    "if y_pred_cnn_classes.shape != y_true.shape:\n",
    "    print(\"Las dimensiones no coinciden. Revisa la configuración del modelo o los datos de entrada.\")\n",
    "else:\n",
    "    # Reporte de clasificación para el modelo CNN\n",
    "    print(\"Reporte de clasificación para el modelo CNN:\")\n",
    "    print(classification_report(y_true, y_pred_cnn_classes))\n",
    "\n",
    "    # Reporte de clasificación para el modelo de Transfer Learning\n",
    "    print(\"Reporte de clasificación para el modelo de Transfer Learning:\")\n",
    "    print(classification_report(y_true, y_pred_transfer_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de etiquetas de las clases deportivas\n",
    "class_labels = ['ajedrez', 'baloncesto', 'boxeo', 'disparo', 'esgrima', 'formula1', 'futbol', 'hockey', 'natacion', 'tenis']\n",
    "\n",
    "# Matriz de confusión para el modelo CNN\n",
    "conf_matrix_cnn = confusion_matrix(y_true, y_pred_cnn_classes)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_cnn, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Matriz de Confusión CNN desde cero')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.show()\n",
    "\n",
    "# Matriz de confusión normalizada\n",
    "conf_matrix_cnn = confusion_matrix(y_true, y_pred_cnn_classes, normalize='true')\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_cnn, annot=True, fmt='.2f', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Matriz de Confusión VGG16 ')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    # Graficar la precisión\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Subplot para la precisión\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validación')\n",
    "    plt.title(f'Evolución de la Precisión - {model_name}')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot para la pérdida\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Validación')\n",
    "    plt.title(f'Evolución de la Pérdida - {model_name}')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "\n",
    "    # Mostrar las gráficas\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Llama a la función para graficar ambos modelos\n",
    "plot_training_history(history_cnn, 'Modelo CNN desde cero')\n",
    "plot_training_history(history_transfer, 'Modelo Transfer Learning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar y preprocesar la imagen para que coincida con el tamaño esperado por el modelo\n",
    "test_image = load_img('C:/Users/mally/Documents/Proyectos/sem_IA2/CNN/single_test/single_test/tenis.jpg', target_size=(128, 128))\n",
    "test_image = img_to_array(test_image)\n",
    "\n",
    "# Normalizar la imagen como en el entrenamiento\n",
    "test_image = test_image / 255.0\n",
    "\n",
    "# Expandir las dimensiones para hacer la predicción (1, 128, 128, 3)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Predecir la clase\n",
    "result = model_transfer.predict(test_image)  # Aquí va el nombre del modelo que crearon\n",
    "\n",
    "# Mostrar los valores predichos (probabilidades para cada clase)\n",
    "print(\"Probabilidades predichas:\", result)\n",
    "\n",
    "# Obtener el índice de la clase con mayor probabilidad\n",
    "predicted_class_index = np.argmax(result)\n",
    "\n",
    "# Obtener el mapeo de clases\n",
    "class_labels = train_generator.class_indices  # Cambiado a train_generator\n",
    "\n",
    "# Invertir el diccionario para obtener las clases por índice\n",
    "class_labels = dict((v, k) for k, v in class_labels.items())\n",
    "\n",
    "# Obtener el nombre de la clase predicha\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "# Imprimir la clase predicha\n",
    "print(f'La imagen pertenece a la clase: {predicted_class_label}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
